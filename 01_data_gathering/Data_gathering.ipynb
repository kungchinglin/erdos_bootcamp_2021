{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed Approach (Using API)\n",
    "\n",
    "The following blocks try to use the API approach, which failed fantastically. I suggest you go to the next section, which works well.\n",
    "\n",
    "You can also use directly PushishiftAPI() without psaw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from pushshift_py import PushshiftAPI\n",
    "import datetime as dt\n",
    "import psaw\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "api = psaw.PushshiftAPI()\n",
    "\n",
    "startEpoch = int(dt.datetime(2020,1,1).timestamp())\n",
    "```\n",
    "    \n",
    "The following block shows how we can get information using pushshift. It shows how we can specify the features and get them. The returned data type is a generator with \"submission\" type as elements, though we can certainly make them into a list.\n",
    "\n",
    "```Python\n",
    "features = ['url','author', 'title', 'subreddit', 'id', 'created', 'score']\n",
    "subreddit = 'NBA'\n",
    "\n",
    "data = api.search_submissions(after=startEpoch,\n",
    "                            subreddit=subreddit,\n",
    "                            filter= features,\n",
    "                            limit=10)\n",
    "\n",
    "for datum in data:\n",
    "    print(datum.id, datum.subreddit, datum.title, datum.author, datum.url, datum.created, datum.score)\n",
    "\n",
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"kxbUr-4PyE7DlQ\",\n",
    "    client_secret=\"Q5rIAPS9IHZ1QgOIkHNY09Y9VMxDsA\",\n",
    "    password=\"PASSWOR\",\n",
    "    user_agent=\"testscript by u/kc_the_scraper\",\n",
    "    username=\"kc_the_scraper\",\n",
    ")\n",
    "```\n",
    "\n",
    "We can use praw to get the post body using the following block.\n",
    "```Python\n",
    "reddit.submission(id='eiev5d').selftext\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "In the following blocks, we create tables and store the information. For some reason, though, the api often acts up and freezes when we loop through the data.\n",
    "```Python\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('redditPosts.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Posts(\n",
    "                id TEXT PRIMARY KEY,\n",
    "                subreddit TEXT,\n",
    "                title TEXT,\n",
    "                author TEXT,\n",
    "                url TEXT,\n",
    "                created int)\n",
    "                ''')\n",
    "\n",
    "features = ['url','author', 'title', 'subreddit', 'id', 'created']\n",
    "subreddit = 'stocks'\n",
    "latest = dt.datetime(2021,5,8).timestamp()\n",
    "earliest = dt.datetime(2020,1,1).timestamp()\n",
    "\n",
    "startEpoch = earliest\n",
    "\n",
    "while startEpoch <= latest:\n",
    "    data = api.search_submissions(after=startEpoch,\n",
    "                            subreddit=subreddit,\n",
    "                            filter= features,\n",
    "                            limit=100)\n",
    "    \n",
    "    for datum in data:\n",
    "        print('Got here 2.')\n",
    "        cur.execute('''INSERT OR IGNORE INTO Posts VALUES (?,?,?,?,?,?)'''\n",
    "                    , (datum.id, datum.subreddit, datum.title, datum.author, datum.url, datum.created))\n",
    "        \n",
    "        currentTime = datum.created\n",
    "    \n",
    "    conn.commit()\n",
    "    if currentTime == startEpoch:\n",
    "        break\n",
    "    startEpoch = currentTime + 1\n",
    "    print(dt.datetime.fromtimestamp(startEpoch))    \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Approach (Getting JSON)\n",
    "\n",
    "The method above is shaky at best. A lot of times the api just freezes. On the other hand, I find using requests much easier. The following code blocks contain what you need for storing reddit data you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime as dt\n",
    "import sqlite3\n",
    "import json\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPushShiftData(after,before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?size=100&after='+str(int(after))+'&before='+str(int(before))+'&subreddit='+str(sub)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "\n",
    "\n",
    "def extractInfo(datum,features):\n",
    "    info = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        info[feature] = datum[feature]\n",
    "    \n",
    "    return info\n",
    "\n",
    "def getLatestTime(data):\n",
    "    return data[-1]['created_utc']\n",
    "\n",
    "def dataStoragePipeline(after, before, sub, conn):\n",
    "    features = ['full_link','author', 'title', 'subreddit', 'id', 'created_utc', 'url']\n",
    "    cursor = conn.cursor()\n",
    "    while after < before:\n",
    "        data = getPushShiftData(after, before, sub)\n",
    "        if not data:\n",
    "            print(\"There is no data anymore.\")\n",
    "            return 1\n",
    "        for datum in data:\n",
    "            cursor.execute('''INSERT OR IGNORE INTO Posts \n",
    "                                VALUES (?,?,?,?,?,?,?)'''\n",
    "                              , (datum['id'], datum['subreddit'], datum['title'], datum['author'], datum['full_link'], datum['created_utc'], datum['url']))\n",
    "        \n",
    "        after = getLatestTime(data) + 1\n",
    "        conn.commit()\n",
    "        print(\"The latest post is submitted at\", dt.datetime.fromtimestamp(after-1))\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''CREATE TABLE IF NOT EXISTS Posts(\n",
    "                id TEXT PRIMARY KEY,\n",
    "                subreddit TEXT,\n",
    "                title TEXT,\n",
    "                author TEXT,\n",
    "                url TEXT,\n",
    "                created int,\n",
    "                ext_link TEXT)\n",
    "                ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('../redditPosts.sqlite')\n",
    "cur = conn.cursor()\n",
    "subreddit = 'GME'\n",
    "end = int(time.time()-86400) #I subtracted by one day, so that we have some buffer.\n",
    "start = dt.datetime(2021,1,1).timestamp()\n",
    "cur.execute('''SELECT MIN(created), MAX(created) FROM Posts\n",
    "                WHERE subreddit = ?''', (subreddit,))\n",
    "datatimes = cur.fetchone()\n",
    "\n",
    "if datatimes[0]:\n",
    "    dataEarly, dataLate = datatimes\n",
    "    if end < dataEarly:\n",
    "        end = dataEarly\n",
    "    elif start < dataLate:\n",
    "        start =dataLate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest post is submitted at 2021-05-09 20:20:45\n",
      "The latest post is submitted at 2021-05-10 04:28:47\n",
      "The latest post is submitted at 2021-05-10 09:19:50\n",
      "The latest post is submitted at 2021-05-10 11:54:26\n",
      "The latest post is submitted at 2021-05-10 14:06:47\n",
      "The latest post is submitted at 2021-05-10 16:53:46\n",
      "The latest post is submitted at 2021-05-10 20:19:43\n",
      "The latest post is submitted at 2021-05-11 03:16:43\n",
      "The latest post is submitted at 2021-05-11 08:32:36\n",
      "The latest post is submitted at 2021-05-11 11:00:37\n",
      "The latest post is submitted at 2021-05-11 13:47:40\n",
      "The latest post is submitted at 2021-05-11 16:16:36\n",
      "The latest post is submitted at 2021-05-11 19:00:22\n",
      "The latest post is submitted at 2021-05-11 21:43:02\n",
      "The latest post is submitted at 2021-05-12 03:51:12\n",
      "The latest post is submitted at 2021-05-12 08:39:21\n",
      "The latest post is submitted at 2021-05-12 10:57:35\n",
      "The latest post is submitted at 2021-05-12 12:45:01\n",
      "The latest post is submitted at 2021-05-12 14:41:58\n",
      "The latest post is submitted at 2021-05-12 17:05:37\n",
      "The latest post is submitted at 2021-05-12 20:34:50\n",
      "The latest post is submitted at 2021-05-13 00:54:26\n",
      "The latest post is submitted at 2021-05-13 06:10:51\n",
      "The latest post is submitted at 2021-05-13 09:02:44\n",
      "The latest post is submitted at 2021-05-13 11:54:40\n",
      "The latest post is submitted at 2021-05-13 14:03:49\n",
      "The latest post is submitted at 2021-05-13 16:32:07\n",
      "The latest post is submitted at 2021-05-13 20:50:08\n",
      "The latest post is submitted at 2021-05-14 04:49:57\n",
      "The latest post is submitted at 2021-05-14 09:21:59\n",
      "The latest post is submitted at 2021-05-14 12:54:21\n",
      "The latest post is submitted at 2021-05-14 16:25:49\n",
      "The latest post is submitted at 2021-05-14 22:23:18\n",
      "The latest post is submitted at 2021-05-15 09:57:49\n",
      "The latest post is submitted at 2021-05-15 14:51:12\n",
      "The latest post is submitted at 2021-05-15 22:09:16\n",
      "The latest post is submitted at 2021-05-16 08:10:37\n",
      "The latest post is submitted at 2021-05-16 14:12:48\n",
      "The latest post is submitted at 2021-05-16 21:57:19\n",
      "The latest post is submitted at 2021-05-17 07:35:47\n",
      "The latest post is submitted at 2021-05-17 11:39:36\n",
      "The latest post is submitted at 2021-05-17 14:18:45\n",
      "The latest post is submitted at 2021-05-17 16:29:11\n",
      "The latest post is submitted at 2021-05-17 21:57:56\n",
      "The latest post is submitted at 2021-05-18 05:39:18\n",
      "The latest post is submitted at 2021-05-18 09:51:45\n",
      "The latest post is submitted at 2021-05-18 12:45:45\n",
      "The latest post is submitted at 2021-05-18 15:05:31\n",
      "The latest post is submitted at 2021-05-18 19:23:32\n",
      "The latest post is submitted at 2021-05-19 02:46:43\n",
      "The latest post is submitted at 2021-05-19 08:41:07\n",
      "The latest post is submitted at 2021-05-19 12:05:15\n",
      "The latest post is submitted at 2021-05-19 15:55:10\n",
      "The latest post is submitted at 2021-05-19 20:57:17\n",
      "The latest post is submitted at 2021-05-20 05:50:16\n",
      "The latest post is submitted at 2021-05-20 11:11:01\n",
      "The latest post is submitted at 2021-05-20 15:00:51\n",
      "The latest post is submitted at 2021-05-20 20:20:33\n",
      "The latest post is submitted at 2021-05-21 04:39:29\n",
      "The latest post is submitted at 2021-05-21 09:28:12\n",
      "The latest post is submitted at 2021-05-21 13:45:15\n",
      "The latest post is submitted at 2021-05-21 19:15:19\n",
      "The latest post is submitted at 2021-05-22 06:12:52\n",
      "The latest post is submitted at 2021-05-22 14:22:42\n",
      "The latest post is submitted at 2021-05-23 00:28:05\n",
      "The latest post is submitted at 2021-05-23 12:28:09\n",
      "The latest post is submitted at 2021-05-23 19:12:54\n",
      "The latest post is submitted at 2021-05-24 03:09:39\n",
      "The latest post is submitted at 2021-05-24 10:32:54\n",
      "The latest post is submitted at 2021-05-24 14:22:15\n",
      "The latest post is submitted at 2021-05-24 20:02:23\n",
      "The latest post is submitted at 2021-05-25 03:31:18\n",
      "There is no data anymore.\n"
     ]
    }
   ],
   "source": [
    "while start < end:\n",
    "    try:\n",
    "        flag = dataStoragePipeline(after = start, before = end, sub = subreddit, conn = conn)\n",
    "        if flag:\n",
    "            break\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by keyboard. Stopping.\")\n",
    "        break\n",
    "        \n",
    "    except:\n",
    "        print(\"Error occurred. Probably due to frequent requests. Will resume working in 1 seconds.\")\n",
    "        time.sleep(1)\n",
    "        cur.execute('''SELECT MIN(created), MAX(created) FROM Posts\n",
    "                        WHERE subreddit = ?''', (subreddit,))\n",
    "        datatimes = cur.fetchone()\n",
    "        \n",
    "        if datatimes[0]:\n",
    "            dataEarly, dataLate = datatimes\n",
    "            if end < dataEarly:\n",
    "                end = dataEarly\n",
    "            elif start < dataLate:\n",
    "                start =dataLate\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''SELECT subreddit,COUNT(*), COUNT(DISTINCT author) FROM Posts\n",
    "                GROUP BY subreddit''')\n",
    "\n",
    "print(cur.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the database by adding links.\n",
    "\n",
    "```Python\n",
    "cur.execute('''ALTER TABLE Posts\n",
    "                ADD COLUMN ext_link TEXT''')\n",
    "\n",
    "def dataUpdatePipeline(after, before, sub, conn):\n",
    "    features = ['url', 'id']\n",
    "    cursor = conn.cursor()\n",
    "    while after < before:\n",
    "        data = getPushShiftData(after, before, sub)\n",
    "        if not data:\n",
    "            print(\"There are no data anymore.\")\n",
    "            return 1\n",
    "        for datum in data:\n",
    "            cursor.execute('''UPDATE Posts\n",
    "                                SET ext_link = ?\n",
    "                                WHERE id = ? AND subreddit = ?'''\n",
    "                              , (datum['url'], datum['id'], sub))\n",
    "        \n",
    "        after = getLatestTime(data) + 1\n",
    "        conn.commit()\n",
    "        print(\"The latest post is submitted at\", dt.datetime.fromtimestamp(after-1))\n",
    "        time.sleep(0.1)\n",
    "\n",
    "def findStartingTime(cursor, subreddit):\n",
    "    cursor.execute('''SELECT MAX(created) FROM Posts\n",
    "                    WHERE subreddit = ? AND ext_link IS NOT NULL''', (subreddit,))\n",
    "    datatimes = cursor.fetchone()\n",
    "    \n",
    "    return datatimes[0]\n",
    "\n",
    "def getYourExistingSubs(cursor):\n",
    "    cur.execute('''SELECT subreddit FROM Posts GROUP BY subreddit ORDER BY COUNT(*) ASC''')\n",
    "\n",
    "    subreddits = list(map(lambda x: x[0], cur.fetchall()))\n",
    "    \n",
    "    return subreddits\n",
    "\n",
    "\n",
    "#This chunk gives you all the subreddits you have scraped up to this point. Makes your life easier.\n",
    "subreddits = getYourExistingSubs(cur)\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    flag = 0\n",
    "    print(\"Start working on {}\".format(subreddit))\n",
    "    end = int(time.time()-86400) #I subtracted by one day, so that we have some buffer.\n",
    "    start = dt.datetime(2021,1,1).timestamp()\n",
    "\n",
    "    dataTime = findStartingTime(cur,subreddit)\n",
    "    if dataTime:\n",
    "        start = max(start, dataTime)\n",
    "    while True:\n",
    "        try:\n",
    "            dataUpdatePipeline(start, end, subreddit, conn)\n",
    "            break\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Interrupted by keyboard. Stopping.\")\n",
    "            flag = 1\n",
    "            break\n",
    "        except:\n",
    "            print(\"Error occurred. Probably due to frequent requests. Will resume working in 1 seconds.\")\n",
    "            dataTime = findStartingTime(cur,subreddit)\n",
    "            if dataTime:\n",
    "                start = max(start, dataTime)\n",
    "            time.sleep(1)\n",
    "    \n",
    "    if flag:\n",
    "        break\n",
    "            \n",
    "            \n",
    "\n",
    "cur.execute('''SELECT subreddit FROM Posts GROUP BY subreddit ORDER BY COUNT(*) ASC''')\n",
    "\n",
    "subreddits = list(map(lambda x: x[0], cur.fetchall()))\n",
    "\n",
    "subreddits\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PRAW to Download Additional Features\n",
    "\n",
    "In this section, we will use praw to scrap post body, score, and upvote_ratio.\n",
    "\n",
    "**Please use your own client_id, secret, etc. since we might be scraping at the same time. I don't want to get this account locked up.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import praw\n",
    "import time\n",
    "import datetime as dt\n",
    "\n",
    "conn = sqlite3.connect('../redditPosts.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"kxbUr-4PyE7DlQ\",\n",
    "    client_secret=\"Q5rIAPS9IHZ1QgOIkHNY09Y9VMxDsA\",\n",
    "    password=\"PASSWOR\",\n",
    "    user_agent=\"testscript by u/kc_the_scraper\",\n",
    "    username=\"kc_the_scraper\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```Python\n",
    "Sub = reddit.submission(id='eiev5d')\n",
    "print(Sub.title, Sub.score, Sub.upvote_ratio)\n",
    "\n",
    "#print(vars(Sub))\n",
    "\n",
    "subreddit = 'options'\n",
    "cur.execute('''SELECT id FROM Posts\n",
    "                WHERE subreddit == ?\n",
    "                LIMIT 10''', (subreddit,))\n",
    "\n",
    "ids = cur.fetchall()\n",
    "\n",
    "\n",
    "tStart = time.time()\n",
    "for i,postId in enumerate(ids):\n",
    "    Sub = reddit.submission(id=postId[0])\n",
    "    print(Sub.selftext, Sub.score, Sub.upvote_ratio, Sub.url)\n",
    "    \n",
    "\n",
    "tEnd = time.time()\n",
    "\n",
    "print(tEnd-tStart)\n",
    "\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS PostBodyAndScore (\n",
    "                    id TEXT PRIMARY KEY,\n",
    "                    body TEXT,\n",
    "                    score INT,\n",
    "                    upvote_ratio FLOAT)''')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timeLookupPRAW(cursor, subreddit):\n",
    "    cursor.execute('''SELECT MIN(P.created), MAX(P.created)\n",
    "                        FROM Posts P JOIN PostBodyAndScore B\n",
    "                        ON P.id = B.id\n",
    "                        WHERE subreddit = ?''', (subreddit,))\n",
    "    return cursor.fetchone()\n",
    "\n",
    "def retrieveID(cursor, num, subreddit, start, end):\n",
    "    cursor.execute('''SELECT P.id\n",
    "                        FROM (SELECT id, created FROM Posts\n",
    "                                WHERE created >= ? AND created <= ? AND subreddit = ?) P LEFT JOIN PostBodyAndScore B\n",
    "                        ON P.id = B.id\n",
    "                        WHERE B.score IS NULL\n",
    "                        ORDER BY P.created ASC\n",
    "                        LIMIT ?''', (start,end, subreddit, num))\n",
    "    \n",
    "    return cursor.fetchall()\n",
    "\n",
    "def storePostBodyAndScore(cursor, reddit, ids):\n",
    "    flag = 0\n",
    "    for i,postId in enumerate(ids):\n",
    "        try:\n",
    "            Sub = reddit.submission(id=postId[0])\n",
    "            cursor.execute('''INSERT OR IGNORE INTO PostBodyAndScore\n",
    "                                VALUES (?,?,?,?)''', (postId[0],Sub.selftext,Sub.score,Sub.upvote_ratio))\n",
    "        except KeyboardInterrupt:\n",
    "            flag = 1\n",
    "            break\n",
    "        except:\n",
    "            print(\"Some error happened. Will abandon this post.\")\n",
    "            continue\n",
    "        \n",
    "    print(\"Finished adding post info for this batch.\")\n",
    "    conn.commit()\n",
    "    return flag\n",
    "\n",
    "def fillOutTable(cursor, reddit, subreddit, start, end, num = 500):\n",
    "    delta = 86400\n",
    "    \n",
    "    _, latest = timeLookupPRAW(cursor,subreddit)\n",
    "    \n",
    "    if latest:\n",
    "        start = max(start,latest)\n",
    "    \n",
    "    while start < end:\n",
    "        ids = retrieveID(cursor, num, subreddit, start, start + delta)\n",
    "        if not ids:\n",
    "            start += delta\n",
    "            print(\"==Finished all posts up to {}==\".format(dt.datetime.fromtimestamp(start)))\n",
    "            continue\n",
    "        \n",
    "        flag = storePostBodyAndScore(cursor, reddit, ids)\n",
    "        if flag:\n",
    "            return flag\n",
    "            \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently working on subreddit GME\n",
      "==Finished all posts up to 2021-05-26 03:31:18==\n"
     ]
    }
   ],
   "source": [
    "#subreddits = getYourExistingSubs(cur)\n",
    "subreddits = ['GME']\n",
    "end = int(time.time()-86400)\n",
    "start = dt.datetime(2021,1,1).timestamp()\n",
    "\n",
    "for subreddit in subreddits:\n",
    "    print(\"Currently working on subreddit {}\".format(subreddit))\n",
    "    flag = fillOutTable(cur, reddit, subreddit, start, end)\n",
    "    if flag:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
