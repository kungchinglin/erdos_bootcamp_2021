{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed Approach (Using API)\n",
    "\n",
    "The following blocks try to use the API approach, which failed fantastically. I suggest you go to the next section, which works well.\n",
    "\n",
    "You can also use directly PushishiftAPI() without psaw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Python\n",
    "from pushshift_py import PushshiftAPI\n",
    "import datetime as dt\n",
    "import psaw\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "api = psaw.PushshiftAPI()\n",
    "\n",
    "startEpoch = int(dt.datetime(2020,1,1).timestamp())\n",
    "```\n",
    "    \n",
    "The following block shows how we can get information using pushshift. It shows how we can specify the features and get them. The returned data type is a generator with \"submission\" type as elements, though we can certainly make them into a list.\n",
    "\n",
    "```Python\n",
    "features = ['url','author', 'title', 'subreddit', 'id', 'created', 'score']\n",
    "subreddit = 'NBA'\n",
    "\n",
    "data = api.search_submissions(after=startEpoch,\n",
    "                            subreddit=subreddit,\n",
    "                            filter= features,\n",
    "                            limit=10)\n",
    "\n",
    "for datum in data:\n",
    "    print(datum.id, datum.subreddit, datum.title, datum.author, datum.url, datum.created, datum.score)\n",
    "\n",
    "import praw\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"kxbUr-4PyE7DlQ\",\n",
    "    client_secret=\"Q5rIAPS9IHZ1QgOIkHNY09Y9VMxDsA\",\n",
    "    password=\"AACAXZDE\",\n",
    "    user_agent=\"testscript by u/kc_the_scraper\",\n",
    "    username=\"kc_the_scraper\",\n",
    ")\n",
    "```\n",
    "\n",
    "We can use praw to get the post body using the following block.\n",
    "```Python\n",
    "reddit.submission(id='eiev5d').selftext\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "In the following blocks, we create tables and store the information. For some reason, though, the api often acts up and freezes when we loop through the data.\n",
    "```Python\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('redditPosts.sqlite')\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Posts(\n",
    "                id TEXT PRIMARY KEY,\n",
    "                subreddit TEXT,\n",
    "                title TEXT,\n",
    "                author TEXT,\n",
    "                url TEXT,\n",
    "                created int)\n",
    "                ''')\n",
    "\n",
    "features = ['url','author', 'title', 'subreddit', 'id', 'created']\n",
    "subreddit = 'stocks'\n",
    "latest = dt.datetime(2021,5,8).timestamp()\n",
    "earliest = dt.datetime(2020,1,1).timestamp()\n",
    "\n",
    "startEpoch = earliest\n",
    "\n",
    "while startEpoch <= latest:\n",
    "    data = api.search_submissions(after=startEpoch,\n",
    "                            subreddit=subreddit,\n",
    "                            filter= features,\n",
    "                            limit=100)\n",
    "    \n",
    "    for datum in data:\n",
    "        print('Got here 2.')\n",
    "        cur.execute('''INSERT OR IGNORE INTO Posts VALUES (?,?,?,?,?,?)'''\n",
    "                    , (datum.id, datum.subreddit, datum.title, datum.author, datum.url, datum.created))\n",
    "        \n",
    "        currentTime = datum.created\n",
    "    \n",
    "    conn.commit()\n",
    "    if currentTime == startEpoch:\n",
    "        break\n",
    "    startEpoch = currentTime + 1\n",
    "    print(dt.datetime.fromtimestamp(startEpoch))    \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Approach (Getting JSON)\n",
    "\n",
    "The method above is shaky at best. A lot of times the api just freezes. On the other hand, I find using requests much easier. The following code blocks contain what you need for storing reddit data you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime as dt\n",
    "import sqlite3\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPushShiftData(after,before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?size=100&after='+str(int(after))+'&before='+str(int(before))+'&subreddit='+str(sub)\n",
    "    r = requests.get(url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "def extractInfo(datum,features):\n",
    "    info = {}\n",
    "    \n",
    "    for feature in features:\n",
    "        info[feature] = datum[feature]\n",
    "    \n",
    "    return info\n",
    "\n",
    "def getLatestTime(data):\n",
    "    return data[-1]['created_utc']\n",
    "\n",
    "def dataStoragePipeline(after, before, sub, conn):\n",
    "    features = ['full_link','author', 'title', 'subreddit', 'id', 'created_utc']\n",
    "    cursor = conn.cursor()\n",
    "    while after < before:\n",
    "        data = getPushShiftData(after, before, sub)\n",
    "        if not data:\n",
    "            break\n",
    "        for datum in data:\n",
    "            cursor.execute('''INSERT OR IGNORE INTO Posts \n",
    "                                VALUES (?,?,?,?,?,?)'''\n",
    "                              , (datum['id'], datum['subreddit'], datum['title'], datum['author'], datum['full_link'], datum['created_utc']))\n",
    "        \n",
    "        after = getLatestTime(data) + 1\n",
    "        conn.commit()\n",
    "        print(\"The latest post is submitted at\", dt.datetime.fromtimestamp(after-1))\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('redditPosts.sqlite')\n",
    "cur = conn.cursor()\n",
    "subreddit = 'wallstreetbets'\n",
    "latest = int(time.time())\n",
    "earliest = dt.datetime(2021,1,1).timestamp()\n",
    "cur.execute('''SELECT MAX(created) FROM Posts\n",
    "                WHERE subreddit = ?''', (subreddit,))\n",
    "databaseLatest = cur.fetchone()[0]\n",
    "\n",
    "if databaseLatest:\n",
    "    start = max(earliest, databaseLatest)\n",
    "else:\n",
    "    start = earliest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-25 11:31:27\n",
      "The latest post is submitted at 2021-01-25 11:34:42\n",
      "The latest post is submitted at 2021-01-25 11:37:59\n",
      "The latest post is submitted at 2021-01-25 11:41:17\n",
      "The latest post is submitted at 2021-01-25 11:45:11\n",
      "The latest post is submitted at 2021-01-25 11:48:33\n",
      "The latest post is submitted at 2021-01-25 11:53:15\n",
      "The latest post is submitted at 2021-01-25 11:57:54\n",
      "The latest post is submitted at 2021-01-25 12:02:10\n",
      "The latest post is submitted at 2021-01-25 12:07:04\n",
      "The latest post is submitted at 2021-01-25 12:11:14\n",
      "The latest post is submitted at 2021-01-25 12:15:57\n",
      "The latest post is submitted at 2021-01-25 12:20:44\n",
      "The latest post is submitted at 2021-01-25 12:25:07\n",
      "The latest post is submitted at 2021-01-25 12:30:13\n",
      "The latest post is submitted at 2021-01-25 12:34:48\n",
      "The latest post is submitted at 2021-01-25 12:39:36\n",
      "Error occurred. Probably due to frequent requests. Will resume working in 1 seconds.\n",
      "The latest post is submitted at 2021-01-25 12:45:51\n",
      "The latest post is submitted at 2021-01-25 12:50:27\n",
      "The latest post is submitted at 2021-01-25 12:55:43\n",
      "Error occurred. Probably due to frequent requests. Will resume working in 1 seconds.\n",
      "Interrupted by keyboard. Stopping.\n"
     ]
    }
   ],
   "source": [
    "while start < latest:\n",
    "    try:\n",
    "        dataStoragePipeline(after = start, before = latest, sub = subreddit, conn = conn)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted by keyboard. Stopping.\")\n",
    "        break\n",
    "        \n",
    "    except:\n",
    "        print(\"Error occurred. Probably due to frequent requests. Will resume working in 1 seconds.\")\n",
    "        time.sleep(1)\n",
    "        cur.execute('''SELECT MAX(created) FROM Posts\n",
    "                        WHERE subreddit = ?''', (subreddit,))\n",
    "        databaseLatest = cur.fetchone()[0]\n",
    "        \n",
    "        if databaseLatest:\n",
    "            start = max(earliest, databaseLatest)\n",
    "        else:\n",
    "            start = earliest\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
